# Parameter Efficient Fine Tuning using Transformers and LoRA
Learn how to fine tune a LoRA. 

## See it live and in action ğŸ“º - Click the image!
<a href="https://youtu.be/D3pXSkGceY0"><img src="https://i.imgur.com/nEfrhIQ.png"/></a>

# Startup ğŸš€
1. Startup on your machine locally by running `uv init`
2. Install all the stuff `uv sync`
3. Place your clean data in `data/instruction.json`
4. Train locally or transition to RunPod (see runpod_setup.md)
5. Deploy using Ollama and Langflow - shown in vid!

# Who, When, Why?

ğŸ‘¨ğŸ¾â€ğŸ’» Author: Nick Renotte <br />
ğŸ“… Version: 1.x<br />
ğŸ“œ License: This project is licensed under the MIT License </br>
