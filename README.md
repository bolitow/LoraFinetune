# Parameter Efficient Fine Tuning using Transformers and LoRA
Learn how to fine tune a LoRA. 

## See it live and in action 📺 - Click the image!
<a href="https://youtu.be/D3pXSkGceY0"><img src="https://i.imgur.com/nEfrhIQ.png"/></a>

# Startup 🚀
1. Startup on your machine locally by running `uv init`
2. Install all the stuff `uv sync`
3. Place your clean data in `data/instruction.json`
4. Train locally or transition to RunPod (see runpod_setup.md)
5. Deploy using Ollama and Langflow - shown in vid!

# Who, When, Why?

👨🏾‍💻 Author: Nick Renotte <br />
📅 Version: 1.x<br />
📜 License: This project is licensed under the MIT License </br>
